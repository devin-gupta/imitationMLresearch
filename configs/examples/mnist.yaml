# Example Config for the MNIST Experiment
# Note that these are not necesarily good hyperparameters, we are just demonstrating how to use
# all the features of the repository

alg: Classification

optim: Adam
optim_kwargs:
  lr: 0.0005
  betas: [0.9, 0.999]

network: MLP
network_kwargs:
  hidden_layers: [256, 256]
  act: ["import", "torch.nn", "Tanh"]

batch_size: 64
collate_fn: null
checkpoint: null

env: Empty
env_kwargs:
  # Specify the IO shape
  observation_shape: [1, 28, 28]
  action_high: 10

dataset: TorchVisionDataset
dataset_kwargs:
  dataset: MNIST
  root: ../datasets/mnist
  train: True
  transform:
   - ["ToTensor", {}]
   - ["Normalize", {"mean": [0.1307], "std": [0.3081]}]
validation_dataset_kwargs:
  train: False

processor: Flatten

schedule: linear_decay
schedule_kwargs:
  total_steps: 10000
  start_step: 5000

trainer_kwargs: # Arguments given to Algorithm.train
  total_steps: 10000 # The total number of steps to train
  log_freq: 25 # How often to log values
  profile_freq: 10 # How often to time different componetns
  eval_freq: 500 # How often to run evals
  max_eval_steps: 100 # Maximum number of steps from the validation dataset, if included
  loss_metric: loss # The validation metric that determines when to save the "best_checkpoint"
  eval_fn: null
  train_dataloader_kwargs:
    batch_size: 64
    num_workers: 2
  validation_dataloader_kwargs:
    num_workers: 0
  benchmark: False # whether or not to enable torch.cuddn.benchmark

seed: 0
